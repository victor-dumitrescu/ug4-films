\documentclass[bsc,frontabs,deptreport,singlespacing,parskip, twoside]{infthesis}
\usepackage{filecontents}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{multirow}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{dirtree}
\usepackage{array}
\usepackage{xcolor, colortbl}
\usepackage{hhline}
\usepackage{chngpage}
\usepackage{pdfpages}

\begin{document}
\long\def\/*#1*/{}

\title{Representing Films as Character Graphs}
\author{Victor Dumitrescu}
\course{Artificial Intelligence and Computer Science}
\project{4\textsuperscript{th} Year Project Report}
\date{\today}

\abstract{
This project sets out to explore a representation of film in terms of its characters and their interactions, bringing together different kinds of textual information, such as screenplays and plot summaries. The work described in this report builds upon the existing literature in computational linguistics concerned with studying characters in films and other works of fiction. The character graph model presented here captures two main aspects of film characters: their personalities and their role in the social network of the narrative. We discuss the process of building and evaluating character graphs and show that they accurately predict the sentiment that film characters express towards each other through dialogue.
}

\maketitle

\section*{Acknowledgements}
I would like to thank Jon Oberlander for his inspiring supervision. I always left his office with something to ponder and excited about the direction that I would take the project next.

I am grateful to Philip Gorinski and Mirella Lapata for providing me with their ScriptBase corpus and their research.

I would also like to thank my family. Were it not for their continuous support, I would not be submitting this project today.

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}

This project started with the idea of constructing a meaningful representation of films, that would enable computational analysis. A film can be seen from many, sometimes complementary, points of view: as a story, comparable to literary narratives \cite{jinks1971celluloid}, as a social network of characters or as a succession of scenes. Engrained in our appreciation of films is the belief that we can abstract common aspects and describe them in terms of these qualities. Film genres are perhaps the most prevalent such example in cinematography. Film going audiences will generally discern between comedies and thrillers, for example. This is of course not limited to films. Kurt Vonnegut \cite{vonnegut2011man} famously illustrates the ``shape of stories", such as ``boy meets girl" and ``Cinderella" as a curve on an \textit{ill fortune - good fortune} axis\footnote{I would like to thank my fellow student Craig Wilkinson for bringing this to my attention.}. A more comprehensive analysis of plot structure (controversially) proposes that most stories can be categorised as one of seven ``basic plots" \cite{booker2004seven}.

Our work will not address this idea directly, but this point will come up again in the final section of the report. Instead, we will take a data-driven approach and explore the ways in which natural language processing and machine learning techniques can be used to derive some insights into the film's narrative. Such representations can be useful in tasks such as automatic summarisation and automatic generation of stories (more details follow in the next chapter). Machine analysis of film scripts is also applied in the entertainment industry, with companies like Epagogix\footnote{www.epagogix.com} claiming to be able to predict the financial success of a film based on its script and suggest improvements that would increase it.

\section{Accomplishments}

The main accomplishments of this project are the development and evaluation of a character graph representation of films. In order to achieve this, we had to design and implement a software system which can:
\begin{itemize}
	\item \textbf{Process the data}, mainly film scripts and plot summaries; we also built tools to acquire data (similarity judgements, metadata) from online sources
	\item \textbf{Construct a probabilistic personality model} of film character, using plot summaries of films
	\item \textbf{Construct a character graph}, using the persona model and character interaction data derived from the film scripts, based on a topic model
	\item \textbf{Construct sentiment trajectories} of films using sentiment analysis techniques and derive labels that can be used to evaluate character graphs through a suite of prediction tasks
	\item \textbf{Construct adequate feature vectors} that enable machine learning tasks; support a set of transformations on these feature vectors in order to carry out the prediction tasks under different conditions
	\item \textbf{Train and evaluate different predictors} and allow parameter optimisation in order to achieve best results
	\item \textbf{Export data}: plotting character graphs and sentiment trajectories, pickling and exporting data to various formats
	\item \textbf{Cope with rapidly changing requirements} and enable easy customisation and addition of features
\end{itemize}

The final snapshot of the code submitted for this project does not capture the full evolution of the system, which had to be constantly changed to implement and test new ideas. Naturally, for many of these tasks, I relied on external libraries, which are referenced when discussing the relevant aspects of the implementation.

\section{Structure of the report}
The report is split into the following chapters:
\begin{itemize}
	\item Chapter 2 (Context and background) introduces and explains ideas from different areas of research, on which the work described in the following chapters is based
	\item Chapter 3 (Aims and hypotheses) presents the central hypothesis of the project and discusses alternative hypotheses which were tried but could not be evaluated
	\item Chapter 4 (Design and implementation) gives a detailed account of how the tasks outlined in the above list were implemented and motivates the design choices that were made
	\item Chapter 5 (Results and discussion) offers detailed results (both successful and unsuccessful) for a number of prediction tasks and assesses the strengths and weaknesses of the character graph representation
	\item Chapter 6 (Conclusions and future work) presents possible extensions of the work done for this project, lists alternative approaches that could have been considered and attempts to place the results in a wider context
\end{itemize}

\chapter{Context and background}
\label{chap:background}

\section{Background work}
Researchers have been interested in computationally analysing narrative fiction for decades. However, this interest has dramatically increased along with the size of raw data available and the ability to store and process ever-growing data sets. In order to establish the basis of the work described in this report, we will highlight some of the previous literature which takes a character-centric point of view when approaching narrative fiction. This line of work is intertwined with many other topics in social network analysis, machine learning and sentiment analysis. We will will also give an overview of the contributions in these area which serve as a basis for our work.

\subsection{The Persona Model}
\label{sec:bg_persona}
One of the things we can think of doing when talking about fictional characters is constructing a representation of their personality. Recent work by David Bamman and others has explored the idea of learning the type or \textit{persona} of film \cite{Bamman2013} and literary \cite{Bamman2014} characters. A character persona is a probabilistic model of a character's personality traits, constructed using certain keywords from a film's plot summary. The cited papers present a number of variations of this model, but we will only describe the \textit{Dirichlet Persona Model} introduced in \cite{Bamman2013}, which is the model that we adapted for this project.

Instead of trying to build these models of character personality using information conveyed in the films themselves, such as dialogue or descriptions from the screenplays, Bamman and his colleagues relied on summaries of films' storyline, written by human editors on Wikipedia. These are usually concise, straightforward descriptions of the events and characters of a film, written in a neutral style. A more detailed discussion of the this data follows in Section~\ref{sec:data sets}. 

A core assumption in their model is that character personalities are primarily represented by three types of words in the plot summaries:
\begin{itemize}
	\item \textbf{Agent verbs:} Verbs for which the character is an agent (e.g. "Starling \textit{travels} to the victim's hometown")
	\item \textbf{Patient verbs:} Verbs for which the character is the patient or object (e.g. "Starling \textit{is led} to the house of Jack Gordon", where \textit{Starling} is a passive nominal subject)
	\item \textbf{Modifiers:} Other words which describe the characters (e.g. "Catherine is still \textit{alive}")
\end{itemize}

The classification of tokens into one of these categories is based on their Stanford Typed Dependencies \cite{de2008stanford}, assigned using the Stanford CoreNLP natural language analysis toolkit\footnote{www.nlp.stanford.edu/software/corenlp.shtml}.

In the \textit{Dirichlet Persona Model} (Figure~\ref{fig:dirichlet_persona}), these words, along with their classification into the 3 categories described above are the only data used to infer the persona of a character. In addition to this model, they propose an augmented one (\textit{Persona Regression}), which also takes some metadata into account, namely the age and gender of the characters (based on the gender and date of birth of the actor portraying that character) and the genre of the film.

\begin{figure}[ht!]
\centering
\begin{tikzpicture}

  % Define nodes
  \node[obs]                      (w) {$w$};
  \node[latent, above=of w]       (z) {$z$};
  \node[obs, right=of z]          (r) {$r$}; 
  \node[latent, above=of z]       (p) {$p$};
  \node[dash, above=of p]         (theta) {$\theta^{(d)}$};
  \node[latent, left=of theta]    (alpha) {$\alpha$};
  \node[dash, left=of w]          (phi) {$\phi^{(z)}$};
  \node[latent, below=of phi]     (gamma) {$\gamma$};
  \node[dash, left=of z]          (psi) {$\psi^{(r)}$};
  \node[latent, above=of psi]     (nu) {$\nu$};

  \edge {z}{w}
  \edge {r}{z}
  \edge {theta}{p}
  \edge {p}{z}
  \edge {alpha}{theta}
  \edge {phi}{w}
  \edge {gamma}{phi}
  \edge {psi}{z}
  \edge {nu}{psi}

  \plate {zwr} {(z)(w)(r)} {$W$};
  \plate {zwrp} {(zwr)(p)} {$E$};
  \plate {zwrp_theta} {(zwrp)(theta)} {$D$};

\end{tikzpicture}
\caption{Graphical model of the \textit{Dirichlet Persona Model} presented in \cite{Bamman2013}. Figure adapted from the one in the paper. As can be seen when comparing with Figure~\ref{fig:lda} below, it extends the LDA model.}
\label{fig:dirichlet_persona}
\end{figure}

\textbf{Graphical models} are useful for representing dependence relationships between random variables and allowing inference to be modelled in terms of graph operations \cite{barber2012bayesian}. In the notation used in Figure~\ref{fig:dirichlet_persona}, nodes are random variables, directed edges represent dependence and a \textit{plate} (rectangle) shows that a certain random variable is replicated a number of times, which is shown in the bottom-right corner of the plate. Nodes that are shaded represent observed variables, while dashed nodes signify that they are \textit{soft evidence} for the variable which depends on them, meaning that the probability mass is divided among different states. This is in contrast to \textit{hard evidence}, where all the probability mass rests in one state. Such a relationship defines a \textit{soft-} or \textit{hard-clustering} of the data. In the former case, data points can be assigned to more than one cluster (e.g. words can belong to more than one topic), while in the latter each point is assigned to a single cluster (e.g. characters have a single persona).

In the generative model shown above, the words $w$ and their role $r$ (\textit{agent}, \textit{patient}, \textit{modifier}) are the observed variables. $W$ is the total number of words. Each word is assigned a topic $z$. Topics $\phi$ are multinomial distributions over words in the vocabulary, with a Dirichlet prior, parameterised by $\gamma$. A persona $p$ is then defined as a set of 3 multinomial distributions $\psi$ over the topics, also with a Dirichlet prior, each parameterised by a $\nu_r$. At the document (film) level, characters' personas are also drawn from a multinomial distribution $\theta$, with a Dirichlet prior, parameterised by $\alpha$. The authors use collapsed Gibbs sampling \cite{griffiths2004finding} to sample the latent topic $z$ of each word and the persona $p$ of each character. Performing this type of inference, while optimising all the hyperparameters on a large data set, is computationally expensive. This is one of the reasons for which we sought to implement and test a simplified version of this model, described in Section~\ref{sec:persona_impl}, which is based directly on an implementation of Latent Dirichlet Allocation.

\subsection{Social networks in narrative fiction}
Another strand of work in this field is concerned with the interactions between characters and how they contribute to defining the story, rather than with their individual personalities. When analysing narratives and the characters that make up the story, we can think of representations that would capture their interactions and the ways in which they affect the story progression. An intuitive representation is that of a social network: a graph in which nodes are used to denote characters and edges represent some kind of interaction between them. Different variations of this model have been proposed.

Extracting and analysing social networks of characters from works of fiction has already proven valuable for a number of applications. In the context of novels, such an approach can be used to answer literary theory questions by enabling the analysis of a large sample of writings, rather then the small subset of well-known works which are usually examined by literary scholars. In their paper \cite{Elson2010}, Elson et al. propose a way in which social networks can be extracted from English novels, for the purpose of answering disputed questions about the correlation between different features of narratives (e.g. first- or third-person perspective, urban or rural setting) and character interactions (e.g. cohesion of the social network). Their approach uses the number and length of quoted speech acts which are inferred to be directed from one character to another to assign a weight to the relationship between the characters.

Others have tried to extend the social network representation beyond quoted speech, also in the context of literary fiction. Notably, Agarwal et al. introduced a wider concept of \textit{social events} \cite{agarwal2010annotation}, meant to include any ``deliberate" interaction not captured by direct speech (e.g., using the notation from the paper with braces for characters and brackets for social events, ``\{He\} [was looking] at \{Tomas\} and [trying to smile]."\footnote{Kundera, Milan. \textit{The Unbearable Lightness of Being}. Translated by Richmond Hoxie. London: Faber \& Faber, 1984.}). The authors proposed a method for automatically extracting social networks from texts and evaluated their result  by comparing their extracted social network from ``The Adventures of Alice in Wonderland" with a gold-standard annotated network, using standard network measures such as degree centrality, betweenness centrality and graph density  \cite{Agarwal2013}.

\subsection{Sentiment analysis}
\label{sec:bg_sentiment}
Sentiment analysis is an active area of research which generally aims to reliably detect the subjective feeling or opinion expressed by someone, usually in writing, towards an idea or topic, a product or another person \cite{varghesesurvey}. In the wake of the ``big data revolution" \cite{mayer2013big}, which generated a substantial interest for many organisations to collect and analyse customer data in order to guide their strategies, sentiment analysis (in this context, often called \textit{opinion mining} \cite{pang2008opinion}) became a key technique for gauging the interest and reactions of the public automatically, on an unprecedented scale. One of the most common sentiment analysis tasks, especially important for industrial applications, is detecting \textit{sentiment polarity}: deciding whether the sentiment expressed (e.g. in a product review, a tweet, a blog post) is positive or negative.

Success in any sentiment analysis task is subject to overcoming many challenges, some of them related to limitations in state-of-the-art natural language processing tools in general, but others specific to the domain. One major obstacle, which seems to be inherent to dealing with subjectivity, is the fact that the accuracy of any such system is limited by the rate of agreement between human judgements. Different studies found that humans will agree on the sentiment polarity of a sentence between 75\% and 90\% of the time \cite{godbole2007large, wilson2005recognizing}. Another difficulty in building automatic sentiment analysis systems is that the way in which people express feelings can be different depending on the context (e.g. product reviews and film reviews) \cite{varghesesurvey}, meaning that systems which perform well in one domain will not necessarily generalise. Sentiment analysis also depends on reliably performing generic NLP tasks, such as named entity recognition, co-reference resolution and dependency parsing\footnote{These concepts will be explained in Chapter~\ref{chap:design}, when discussing the implementation of the text processing pipeline for the project.}, which are still not completely solved problems. It also struggles with ambiguous uses of language, such as sarcasm or words which change their meaning depending on the context in which they are used.

\subsubsection{Approaches to automatic sentiment analysis}

There are a number of general-purpose and domain-specific sentiment analysis systems in wide use today.

Sentiment analysis is usually broken down into different levels of granularity and treated differently in each case \cite{liu2012sentiment}:
\begin{itemize}
	\item \textbf{Phrase level} Many sentiment analysis systems rely on gold-standard corpora which annotate large numbers of words and phrases with a sentiment valence.
	\item \textbf{Sentence level} In the most basic form, the sentiment of a sentence can be computed by summing the sentiment valences of its constituents. However, the best sentiment analysis models will exploit the structure of sentences in more sophisticated ways \cite{socher2013recursive}.
	\item \textbf{Document level} Assessing the sentiment of a document as a whole (i.e. a text composed of multiple sentences, such as a blog post) assumes that it expresses a single, coherent point of view about a single topic. Therefore, it has limited applicability, since this assumption often does not hold in practice.
\end{itemize}

\subsubsection{The VADER sentiment analysis system}

For this project, we used VADER (\textit{Valence Aware Dictionary for sEntiment Reasoning}) \cite{hutto2014vader}, a rule-based sentiment analysis system, geared towards analysing social media content. It is implemented in Python and it is available as an open-source project under the MIT licence\footnote{www.pypi.python.org/pypi/vaderSentiment}.

We chose to use VADER because it is a simple model which uses a combination of features (a gold-standard sentiment valence lexicon and learned composition rules that correlate with sentiment expression in different contexts) to determine the overall sentiment of a sentence. It achieves state-of-the-art results, comparable to more sophisticated and expensive models such as the recursive deep model of Socher et. al.\cite{socher2013recursive}. Its accuracy is significantly better than those which compute the sentence-level sentiment as the sum of the sentiment of its parts.


\subsection{Sentiment analysis in the study of fiction}
Among the first efforts in using sentiment analysis on literary fiction is the work of Alm et al. \cite{alm2005emotions}, on analysing children's fairy tales. The purpose of their research was to improve text-to-speech synthesis, by taking emotional cues from the story into account, similar to how a person would read a story to a child. They also explore the role of ``emotion trajectories", which model the distribution of emotions over the course of the narrative, in the evolution of stories written for children \cite{alm2005emotional}.

Building on this previous work and on that of Elson et al. \cite{Elson2010} (discussed above), Elsner uses sentiment trajectories of individual characters and a representation of relationships between them in order to compare the high-level plot structure of novels, using a kernel to do one-to-one comparisons of characters between different novels. \cite{elsner2012character}.

Most of the previous literature in this area focuses on literary fiction rather than films. An example of recent work that employs, among other techniques, social network analysis and sentiment analysis is Gorinski and Lapata's paper \cite{Gorinski2015} on film scene extraction for the purpose of automatic film summarisation. For this work, they compiled the ScriptBase corpus, on which this project is also based. They extract the social network of a film using the number of character-to-character interactions as edge weights, which is also the approach used in this project. When constructing chains of scenes, they take into account a ``diversity" metric, which is computed in terms of the difference, from one scene to the next, in the set of participating characters and the overall sentiment of each scene. In this project, however, we are looking at character-to-character sentiment rather than in scenes as a whole.

This idea has been previously explored, recently by Nalisnick et al. \cite{Nalisnick2013}, in their paper on character-to-character sentiment analysis in Shakespeare's plays. They exploit the semi-structured format of theatrical plays and introduce a simple set of assumptions to identify the speaker and listener of each line of dialogue. For this project, we adapted their approach in order to fit film scripts rather than plays in constructing our sentiment trajectories. A more detailed discussion of their work follows in Section~\ref{sec:trajectories}, where we explain the design and implementation of these trajectories.

\section{Data sets} \label{sec:data sets}
We have mainly used two data sets to support the work done for this project. They have both been compiled as part of previous research efforts focused on computationally analysing films.

\subsection{The CMU Movie Summary Corpus}
A collection of 42,306 Wikipedia plot summaries of films, along with metadata extracted from Freebase. The summaries are processed using the \textit{Stanford CoreNLP} tools, specifically tagging, parsing, named entity recognition and coreference resolution. Words of interest are also annotated with WordNet supersenses. Figure~ \ref{fig:lambs_summary} shows the beginning of the plot summary for \textit{The Silence of the Lambs}.

\begin{figure}[h!]
\centering
\begin{minipage}{14.5cm}
\begin{Verbatim}[frame=single]
Clarice Starling  is pulled from her training at the FBI Academy 
at Quantico, Virginia, by Jack Crawford  of the Bureau's Behavioral
Science Unit. He assigns her to interview Hannibal Lecter, a 
former psychiatrist and incarcerated cannibalistic serial killer, 
whose insight might prove useful in the pursuit of a serial killer
nicknamed "Buffalo Bill", who skins his female victims' corpses.

Starling travels to the Baltimore State Hospital for the Criminally 
Insane, where she is led by Frederick Chilton (Anthony Heald) to 
Lecter's solitary quarters. Although initially pleasant and 
courteous, Lecter grows impatient with Starling's attempts at 
"dissecting" him and rebuffs her.
\end{Verbatim}
\end{minipage}
\caption{Fragment of the Wikipedia plot summary for \textit{The Silence of the Lambs} (1994)}
\label{fig:lambs_summary}
\end{figure}


Wikipedia's guidelines for editors writing plot summaries state that it should be ``an overview of the film's main events, [avoiding] minutiae like dialogue, scene-by-scene breakdowns, individual jokes, and technical detail."\footnote{www.en.wikipedia.org/wiki/Wikipedia:How\_to\_write\_a\_plot\_summary} More details on how the plot summaries in the CMU Move Summary Corpus are processed and annotated can be found in Section~\ref{sec:processing_cmu}, which discusses our use of the corpus. Appendix A provides a step-by-step example to illustrate this process.


\subsection{ScriptBase}
ScriptBase \cite{Gorinski2015} is a corpus of 1,276 film scripts, along with IMDB and Wikipedia metadata, including cast list, (unprocessed) plot summaries and taglines. Of these scripts, 127 have been similarly processed using the \textit{Stanford CoreNLP} pipeline and standardized as XML files following a set schema.

The 127 processed films were the staring point for us to construct the character graphs. Of these films, 96 had a correspondent in the CMU Movie Summary Corpus and we were able to construct a persona model for at least one of the characters in 86 of these films. Thus, our core data set was constructed based on these 86 films. More details on the corpus and constructing the data set follow in Chapter~\ref{chap:design}. Figure~\ref{fig:lambs_script} shows an excerpt from the script of \textit{The Silence of the Lambs}

\begin{figure}[h!]
\centering
\begin{minipage}{13.2cm}
\begin{Verbatim}[frame=single]
NEW ANGLE - REVEALS CLARICE

now wearing a more feminine skirt suit. Hair neatly coiled, 
elegant shoulder bag, briefcase. He has rudely left her 
standing.

            CHILTON
Will you be in Baltimore overnight...? 
Because this can be quite a fun town, 
if you have the right guide.

Clarice tries, unsuccessfully, to hide her distaste for him.

            CLARICE
I'm sure it's a great town, Dr. 
Chilton, but my instructions are to 
talk to Lecter and report back this 
afternoon.

            CHILTON
     (pause, sourly)
I see.
     (beat)
Let's make this quick, then. I'm 
busy.

                                                  CUT TO:

INT. ASYLUM CORRIDOR - UPPER FLOOR - DAY

Clarice flinches as a heavy steel gate CLANGS shut behind 
her, the bolt shooting home. Chilton walks ahead of her.
\end{Verbatim}
\end{minipage}
\caption{Fragment of the script for \textit{The Silence of the Lambs} (1994)}
\label{fig:lambs_script}
\end{figure}



\chapter{Aims and hypotheses}
\label{chap:aims}

The aims of this project were to find and experiment with different representations of films and to identify what kinds of features can be captured by these representations. A big part of the work done for the project was directed at probing these possibilities, by constructing variations of the character graph representation and measuring their effectiveness using a number of prediction tasks. Inevitably, not all of them produced significant results or were even practical to test. Along with our main hypothesis, we will present some of the work which did not yield any fruit and attempt to discuss the underlying reasons.

\section{Inter-character sentiment}
In one sentence, the main hypothesis is that the character graph representation introduced in this project can be used to predict inter-character sentiment. As discussed in Chapter~\ref{chap:background}, there have been previous attempts to link character social networks and some form of sentiment analysis. By using character personas and a measure of the strength of two characters' relationship to predict aspects of the sentiment in speech acts directed from one character to the other, we can bring together two distinct aspects of film narratives and show that they correlate. On the level of the text resources used, we link the raw dialogue of a film (from the film scripts) and the human-produced plot summaries. On the narrative level, we correlate the \textit{actions} of characters (as described in the plot summaries) with their utterances. Success in this task could potentially be useful in other NLP problems, such as automatic summarisation and generation of narratives. Details of how we constructed the character graphs follow in Chapter~\ref{chap:design} on design and implementation, while the full discussion of results obtained is the subject of Chapter~\ref{chap:results}.

\section{Alternative hypotheses}

This section presents two of the hypotheses which were strongly considered and explored and discusses why they were not feasible to test. More details on the design and implementation of the code written to investigate them are included in the next chapter (Section~\ref{sec:alt_hyp_implementation}).

\subsection{TV Tropes}
\label{sec:alt_tropes}
TV Tropes\footnote{www.tvtropes.org} is an online public wiki maintaining a collection of ``tropes", such as plot devices, dialogue commonalities, motifs and other conventions or stereotypes that are widely used in film and television series. More recently it has also grown to include literature, video games and other media. Apart from collecting the tropes themselves, TV Tropes maintains pages for numerous individual films and other works of fiction which include a list of the tropes that they feature.

TV Tropes have been used in past research efforts, most notably by Bamman et al. \cite{Bamman2013}, as part of one of the evaluation measures for their film persona models introduced in Section~\ref{sec:bg_persona}. They selected 72 tropes which correspond to character types (e.g. \textit{The Hardboiled Detective}) and manually annotated 501 characters. They attempted to recover gold-standard clusters of characters as determined by their trope assignment and used an information-theoretic measure to assess the discrepancy between these and the latent persona clusters.

We considered the possibility of evaluating our character graph representations on the basis of being able to capture different kinds of relationships between 2 or more characters (e.g. one of the \textit{Triang Relations}\footnote{www.tvtropes.org/pmwiki/pmwiki.php/Main/TriangRelations} or \textit{Four-Man Band}). For this purpose, we have implemented a scraper in order to collect the tropes associated with the films in our data set. Table~\ref{table:tropes} shows the most common 20 tropes and the number of films which feature them.

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}[h!]
\centering
\begin{tabular}{|c||L{4.5cm}|l|c|c||L{4.5cm}|l|}
\cline{1-3} \cline{5-7}
1 & Shout-Out & 54 & & 11 & Berserk Button & 22 \\ \cline{1-3} \cline{5-7}
2 & Deadpan Snarker & 31 & & 12 & Cluster F-Bomb & 19 \\ \cline{1-3} \cline{5-7}
3 & Lampshade Hanging & 27 & & 13 & Butt Monkey & 18 \\ \cline{1-3} \cline{5-7}
4 & Chekhov's Gun & 27 & & 14 & Large Ham & 18 \\ \cline{1-3} \cline{5-7}
5 & Running Gag & 26 & & 15 & Badass & 18 \\ \cline{1-3} \cline{5-7}
6 & Oh, Crap & 25 & & 16 & Ax-Crazy & 18 \\ \cline{1-3} \cline{5-7}
7 & Brick Joke & 24 & & 17 & Title Drop & 17 \\ \cline{1-3} \cline{5-7}
8 & Precision F-Strike & 23 & & 18 & Word Of God & 17 \\ \cline{1-3} \cline{5-7}
9 & Foreshadowing & 23 & & 19 & Meaningful Name & 16 \\ \cline{1-3} \cline{5-7}
10 & Black Comedy & 22 & & 20 & Even Evil Has Standards & 16 \\ \cline{1-3} \cline{5-7}
\end{tabular}
\caption{The 20 most frequent TV Tropes and the number of films which feature them (out of the 86 films in the collection)}
\label{table:tropes}
\end{table}

None of these 20 tropes describe character relationships in any way. Most of them capture plot devices, features of speech or stylistic choices. Lacking a significant number of films (all other tropes are featured in less than 16 films) which include any kind of stereotypical and easily identifiable character relationships, we were not able to use TV Tropes to evaluate the character graph representation.

\subsection{Film similarity}
\label{sec:hyp_film_sim}
Another evaluation measure considered for assessing the usefulness of character graphs was the ability to compare them in order to predict film similarity, as judged by human raters. In principle, we would expect to obtain at least some degree of accuracy, although film similarity can depend on contextual information beyond the dialogue or narrative of a film. Of course, most recommender systems determine film similarity based on users' reviews and activity and on metadata (e.g. genre, year, cast, crew) rather than the content of the films themselves \cite{said2010putting}.

Some of the reasons for which we were not able to test this hypothesis are the small size of our film corpus and the fact that the films were quite diverse, resulting in very few similarity relations between them. Furthermore, graph comparison is a difficult problem, highly specific to the domain. There are no universal solutions, but some work has been done on comparing graphs of social networks \cite{macindoe2010graph}. A fuller discussion of our attempts to investigate this hypothesis follows in the chapter on design and implementation (Section~\ref{sec:test_film_sim}). This is because it relies on details and explanations given in subsequent sections.

\chapter{Design and implementation}
\label{chap:design}
This chapter details the components of the software pipeline implemented as part of the project to acquire and process data, construct character graphs and carry out experiments. The system was primarily programmed using Python 2.7.6. The language was chosen for the terse and flexible style of programming that it enables and for its adoption in the NLP community, having a large ecosystem of libraries and tools developed around it. Some parts concerned with parsing script files and gathering film similarity judgements from Rotten Tomatoes were implemented using F\# 3.1. This choice was made on the basis of F\#'s smooth integration with external data sources and ease of working with schematised data.

The structure of the Python project is shown in Figure~\ref{fig:dirtree}. Directories and source files in the root directory correspond to the major areas of implementation, which can roughly be categorised as follows (by directory/file):
\begin{itemize}
	\item \texttt{\textbf{topics}}:  Extract and filter plot summary data, run Latent Dirichlet Allocation and pickle the resulting topic distributions
	\item \texttt{personas.py}: Define a \texttt{Persona} class, compute the persona for each character and pickle them
	\item \texttt{graphs.py}: Construct character graphs and implement various functions for working with them (e.g. filter nodes/edges)
	\item \texttt{film.py}: Define a \texttt{Film} class, which stores the character graphs and matches them with various pieces of metadata (e.g. title, year, names of characters)
	\item \texttt{\textbf{sentiment}}: Extract speech acts from processed film scripts and construct sentiment trajectories between pairs of characters
	\item \texttt{\textbf{prediction}}: Implements various predictors and tools for feature engineering and running repeated experiments with different configurations
	\item \texttt{\textbf{utils}}: Processing raw data, pickling and other utility functions required throughout the project
\end{itemize}

The names of the rest of the source files give a good indication of what functionality they implement and should serve as a guide for exploring the source code.

\hspace{.1in}
\begin{figure}[ht!]
\dirtree{%
.1 \textbf{python}.
.2 \textbf{topics}.
.3 acquire\_vocab.py.
.3 filter\_genres.py.
.3 model\_topics.py.
.2 \textbf{sentiment}.
.3 trajectories.py.
.2 \textbf{prediction}.
.3 decision\_tree.py.
.3 random\_forest.py.
.3 svm.py.
.3 lin\_regression.py.
.3 ridge\_regression.py.
.3 regression\_tree.py.
.3 svm\_regression.py.
.3 features.py.
.3 predict.py.
.2 \textbf{utils}.
.3 filter\_summaries.py.
.3 process\_summaries.py.
.3 process\_scripts.py.
.3 store\_sentiment.py.
.3 misc.py.
.2 films.py.
.2 personas.py.
.2 graphs.py.
.2 compare.py.
.2 run\_topic\_model.py.
}
\caption{General structure of the Python code written for the project}
\label{fig:dirtree}
\end{figure}

The code for constructing the Rotten Tomatoes film similarity graph was written in F\#. Its structure is outlined in Figure~\ref{fig:fsharpdirtree}.

\begin{figure}[th!]
\dirtree{%
.1 \textbf{\texttt{rtgraph}}.
.2 Graph.fs.
.2 ApiCalls.fs.
.2 ToGEXF.fs.
.2 Main.fs.
}
\caption{General structure of the F\# code written for the project}
\label{fig:fsharpdirtree}
\end{figure}

\section{The Character Graph}
In order to construct the character graph for a film we need to find a meaningful representation of the relationships between characters (the nodes in our graph). For simplicity, we can start by trying to find a numerical measure, which would correspond to edge weights in an undirected weighted graph. The measure we have used in this project is the number of scene co-occurrences between two characters. Thus, the weight of an edge between two characters represents the number of scenes in which those characters appear together. As a consequence, characters which never interact in the film will be disconnected in the graph. 

Apart from simplicity, choosing this measure can enable us to reliably discover the main characters of a film using a graph centrality measure. However, scene co-occurrence does not give any insights into the \textit{nature} of the interaction between characters, but a high co-occurrence is likely to indicate that a relationship is important for the development of the story. Our approach is similar to that of Gorinski et al. \cite{Gorinski2015}, although they go beyond degree centrality for identifying main characters.

\subsection{Processing ScriptBase screenplays}
\label{sec:processing_scriptbase}
To ease the task of extracting the number of character to character scene co-occurrences, we restricted the set of films to the 127 which have already been processed in ScriptBase. A processed script is an XML file which, apart from the standard CoreNLP annotations, also places every sentence inside a numbered \texttt{scene} tag. It also distinguishes between stage directions and speech, annotating the name of the speaker(s) in the latter case.

Thus, it is straightforward to parse the XML document and, for each pair of characters, record the number of scenes in which they both have at least one line of speech. Note that although this approach ignores characters which might be present in a scene but do not speak, in practice we do not expect this to significantly influence any of the results.

\subsection{Constructing character graphs}
For generating and storing character graphs, we have chosen the GEXF format\footnote{The GEXF file format, \textit{www.gexf.net}}, a standard XML format for describing general network structures. As described above, we extract all the character names from the script and use them as the set of nodes in the graph. We add a weighted edge for all the recorded interactions between characters. As a result, each of the 127 films in the data set will be represented with a GEXF file.

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{figures/clarice_graph}
	\caption{The character graph of \textit{The Silence of the Lambs} (1991), centred around the protagonist, Clarice Starling. Red vertices are those adjacent to Clarice (characters with which she interacts). Edge width is proportional to the number of scene co-occurrences between two characters.}
\end{figure}

Once the character graphs are constructed, the nodes can be annotated with the persona information. Note that although the character graphs are likely to contain a node for every character in the plot (a small variation can occur due to faults in the preprocessing of scripts), personas will be computed for 2-5 main characters. Again, due to the limitations of the standard NLP tools (most notably coreference resolution), this number can be lower, sometimes even 0. To work with annotated graphs in our project, we have used the Python \texttt{networkx} library\footnote{www.networkx.github.io}, which can be used for complex graph modelling and implements many useful graph algorithms and network measures.


\section{The Persona Model}
\label{sec:persona_impl}
We implemented a simplified version of the Dirichlet Persona Model introduced by Bamman et al. \cite{Bamman2013} and described in Section~\ref{sec:bg_persona}. 

\subsection{Processing the CMU script summaries}
\label{sec:processing_cmu}
The CMU corpus provides both the raw plot summaries and a file containing all the processed summaries, which can be used as input for the pipeline described in their paper. We have designed ours to work with the same input format, enabling us to use the data from this corpus without any additional preprocessing, apart from filtering out the films for which we have not constructed a character graph. This includes the majority of films in the corpus. Out of the 127 films from ScriptBase, 96 also have a processed plot summary. The films in this intersection are the starting point in our processing pipeline.

\begin{figure}[h]
\centering
\begin{minipage}{14.5cm}
\begin{Verbatim}[frame=single]
30006 /m/0k6g8c:t0.1.0:verb.contact:pull:a:agent /m/0k6g8p:t2.2.0
:verb.stative:lead:a:agent /m/0k6g86:t3.0.0:verb.change:grow:a:ns
ubj /m/0k6g86:t3.2.0:verb.communication:rebuff:a:nsubj (...)
\end{Verbatim}
\end{minipage}
\caption{An entry (corresponding to a film) in the input file used for the work described in \cite{Bamman2013}, which we filtered and used as input for this project.}
\label{fig:bamman_input}
\end{figure}


The filtered film data file contains, for each film and each character mentioned in the plot summary, a list of words tagged as being either \textit{agent}, \textit{patient} or \textit{modifier} words (as detailed in Section~\ref{sec:bg_persona}). These words are selected into one of these types on the basis of their Stanford typed dependencies, as determined using the CoreNLP parser. Each of the three types described above corresponds to a list of dependencies and all tokens tagged with those dependencies will be assigned a type. In the final input file, in addition to the word lemma, its type (agent, patient, modifier) and its dependency tag, the word tuples for each character also contain the part of speech and the WordNet supersense (which are broad semantic categories, such as \textit{noun.shape} or \textit{verb.motion}). As shown in Figure~\ref{fig:bamman_input}, each line starts with the identifier of the film, followed by a list of (colon-separated) tuples, containing this data. The first item in the tuple will either be a Freebase\footnote{Freebase (www.freebase.com) is a Google-owned, community-driven repository of structured data, containing numerous topics, entities and connections between them. Each entity has a unique ID.} identifier or have the format \texttt{eXX} (where \texttt{XX} stands for a 2 digit number) for characters and other entities which could not be matched with Freebase entities. The full preprocessing pipeline, with a sample sentence as a running example is outlined in Appendix~\ref{appendix:cmu}. Note that Bamman et al. provide both the processed data and the code required to produce it\footnote{www.ark.cs.cmu.edu/personas}.


\subsection{Constructing the persona model}
In this project we implement a simplified version of the Dirichlet Persona Model. The original model was implemented as a generative model. In the paper, a persona is defined as a set of 3 distributions (corresponding to agent, patient and modifier words) over topics, which are in turn distributions over words, similar to topics in Latent Dirichlet Allocation.

We have opted to use an implementation of the standard Latent Dirichlet Allocation \cite{Blei2003} algorithm, using the \texttt{lda 0.3.2} Python package\footnote{ www.pypi.python.org/pypi/lda} in order to derive the latent topics in our vocabulary and use a simple maximum likelihood estimation method to compute each character's 3 distributions over these latent topics.

\begin{figure}[h!]
\centering
\begin{tikzpicture}

  \node[obs]                               (w) {$w$};
  \node[latent, above=of w]                (z) {$z$};
  \node[dash, above=of z]              (theta) {$\theta^{(d)}$};
  \node[latent, left=of theta]        (alpha) {$\alpha$};
  \node[dash, left=of w]                 (phi) {$\phi^{(z)}$};
  \node[latent, below=of phi]          (gamma) {$\gamma$};

  \edge {z}{w}
  \edge {theta}{z}
  \edge {alpha}{theta}
  \edge {phi}{w}
  \edge {gamma}{phi}

  \plate {zw} {(z)(w)} {$W$};
  \plate {zwt} {(zw)(theta)} {$D$};

\end{tikzpicture}
\caption{Graphical model for Latent Dirichlet Allocation (LDA)}
\label{fig:lda}
\end{figure}

One of the advantages of this frequentist approach is that the only parameter that we need to optimise in the model is the number of topics computed by the LDA algorithm. Since the number of films in our collection is small and largely restricted to two genres (thriller and comedy), we expect the number of meaningful topics to also be reasonably small. Therefore we have tried to manually vary this parameter and select the value which seems to intuitively give a complete coverage of topics, without having different topics which seem very close. We have used the highest probability words in each topic distribution as a guide and settled on having 10 topics (Figure~\ref{fig:lda}).

\begin{figure}[h]
\begin{minipage}{14.6cm}
\begin{Verbatim}[frame=single]
1: leave meet reveal wife marry ask decide call woman want husband
2: father mother child son marry parent live brother daughter woman
3: kill shoot take man officer arrest escape kidnap detective lead
4: kill rescue escape save arrive bring reveal destroy vampire fly
5: tell be ask see say call friend talk show start mother invite
6: run get chase catch throw grab pull knock hit fall stop walk use
7: friend girl meet student play teacher end girlfriend school 
8: find take follow driver travel join reach fly track locate sail
9: kill man shoot attack killer stab die dead confront attempt
10: boy win play have owner manager hero uncle narrator stooge
\end{Verbatim}
\end{minipage}
\caption{Most representative words for each of the 10 topics learned by LDA on all the comedies and thrillers in the CMU Movie Summary Corpus, ranked by probability.}
\label{topics}
\end{figure}

Much like the Dirichlet Persona Model described in Section~\ref{sec:bg_persona}, LDA is a generative model which assumes that a document is created by first choosing the topics, and then sampling words from the topic distributions. Having these 10 topics distributions over words in the vocabulary, we construct, for each character, a set of 3 distributions over these topics, by multiplying the number of different occurrences of a certain word lemma with the probability of that lemma in a certain topic, then normalising. It is then easy to compare the persona of two different characters, by using any distance measure, such as Euclidean distance.

\section{Sentiment trajectories}
\label{sec:trajectories}
The sentiment trajectory of a film is the ordered sequence of speech acts, in which one (and only one) character (\textit{the speaker}) is assumed to speak to one (and only one) other character (\textit{the receiver}), which is different than the speaker. The sentiment of each line of dialogue is independently computed, using VADER. Each point in the sentiment trajectory is then a tuple of \textit{(speaker, receiver, sentiment)}, which we will call a \textit{sentiment event}.

The ScriptBase corpus includes sentiment valence annotations for each word in a script, using the AFINN-96 sentiment corpus \cite{nielsen2011new}, which assigns a value between -5 and +5 to 1468 English words and phrases. The sentiment of a text fragment can then be computed by summing the values of its constituents. We chose to use the VADER corpus instead because it also takes the structure of sentences into account, instead of relying exclusively on individual sentiment valences.

In order to construct a sentiment trajectory for each film in the data set, we traverse the XML script (described in Section~\ref{sec:processing_scriptbase}), extract every scene and the names of the characters which take part and every line of dialogue, together with its speaker. The speaker of each utterance is marked in the script, but we need to identify a receiver. We will follow the assumptions introduced in \cite{Nalisnick2013}:
\begin{itemize}
	\item For the first line of dialogue in the scene, assume the receiver is the next speaker.
	\item For all the following lines, assume the receiver is the previous speaker.
\end{itemize}

Although these assumptions (and those introduced in the first paragraph, restricting dialogue between exactly two distinct characters) are quite “na\"{\i}ve, in practice they tend to hold reasonably well, especially since many scenes only feature 2 participants.

Another assumption that we make when using the sentiment trajectories as representations of two characters' relation is that the sentiment expressed in a speaker's utterance is directed towards the receiver. Of course, this might not always be the case. To remedy this, we could attempt to parse the typed dependencies from each sentence and only use the phrase-level sentiment from phrases which have a relevant dependency relations with the receiver (e.g. \textit{nsubj}, \textit{dobj}, \textit{ref}, ...), coupled with coreference resolution to match characters with identifiers such as ``you", ``your", etc. However, this approach could result in far sparser sentiment trajectories. In practice, although a line of dialogue might not be directly referring to the other party, its sentiment can often be taken as an indicator of the speaker's relationship with the receiver. To minimise the impact of outliers resulting from either a failure of the sentiment analysis or the breaking of one of the assumptions introduced above, we will only use aggregate measures of the sentiment trajectory in our evaluation tasks, discussed in Chapter~\ref{chap:results}.

Once constructed, sentiment trajectories can be plotted as time series. For clarity, we will only include one character relationship per plot. Note, however, that relationships are directional, since the sentiment that each character expresses towards the other will differ. The plots shown here are illustrative of certain aspects related to film genre or the dynamic between the characters. 

Figure~\ref{fig:thrillers} shows the sentiment trajectories of two thrillers. Both the main characters are allies in the story, but the protagonists of \textit{Frozen River} have a very uneasy alliance, while those in \textit{The Book of Eli} are sympathetic towards each other. Figure~\ref{fig:romcoms} shows the sentiment of the protagonists of two romantic  comedies. One of them (\textit{Midnight in Paris}) has a ``happy ending", with the implication that the characters' relationship will continue, while the other one (\textit{I Love You Phillip Morris}) ends on a sadder note, with the characters having a fight towards the end.

One thing to notice is that although the sentiment trajectories seem to capture the ``attitude" that two characters exhibit towards each other, they are usually not enough to characterise the nature of their relationship at a more meaningful level. A positive sentiment will not be enough to distinguish between, for example, friendship, mentor-student or romantic relationships, while a negative sentiment could mean both that the characters are at odds with each other or that they are allies, but not on friendly terms. They do, however, give a gist of a film and can capture major plot points. 

\begin{figure}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
	\centering
	\includegraphics[scale=0.43]{figures/y_thrillers}
	\caption{Sentiment trajectories of the two most central characters in the thrillers \textit{Frozen River} (2008) and \textit{The Book of Eli} (2010)}
	\label{fig:thrillers}
\end{adjustwidth}
\end{figure}

\begin{figure}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
	\centering
	\includegraphics[scale=0.43]{figures/y_romcoms}
	\caption{Sentiment trajectories of the two most central characters in the romantic comedies \textit{Midnight in Paris} (2011) and \textit{I Love You Phillip Morris} (2009)}
	\label{fig:romcoms}
\end{adjustwidth}
\end{figure}

\begin{figure}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
	\centering
		\includegraphics[scale=0.43]{figures/chasing_amy1}
		\includegraphics[scale=0.43]{figures/chasing_amy2}
	\caption{Sentiment trajectories of the three most central characters in the romantic comedy 
\textit{Chasing Amy} (1997)}
	\label{fig:chasing_amy}
\end{adjustwidth}
\end{figure}

\begin{figure}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
	\centering
	\includegraphics[scale=0.43]{figures/y_contradictions}
	\caption{Sentiment trajectories of the two most central characters in the films \textit{Analyze That} (2002) and \textit{Gothika} (2003)}
	\label{fig:contradictions}
\end{adjustwidth}
\end{figure}

For example, the sentiment trajectories in Figure~\ref{fig:chasing_amy} illustrate the sentiment between the main characters of the romantic comedy \textit{Chasing Amy}. In the film, Holden and Banky are two long-time friends. When the two meet Alyssa, she immediately piques Holden's interest. Although she only sees him as a friend, he pursues her throughout the film and they end up being together. Banky's distrust of Alyssa, however, becomes the main source of conflict between him and Holden. The 3 sentiment trajectories are consistent with our intuitive expectations of such a story and the feelings that the characters would express towards each other in these situations.

In the examples given so far, the sentiment between the characters has largely been reciprocated (both characters expressing either positive or negative sentiment). We can also examine relationships for which that is not the case. The comedy \textit{Analyze That} (left plot in Figure~\ref{fig:contradictions}) is centred around the relationship between an organised crime boss and his psychiatrist. The humour of the films heavily relies on the contradictory and bizarre relationship between the two\footnote{This claim is supported by different reviews, for example one for the film \textit{Analyze This} (1999), of which \textit{Analyze That} is a sequel, by film critic Roger Ebert (www.rogerebert.com/reviews/analyze-this-1999).}. As it can be seen from the plot, the sentiment expressed by one character towards the other is almost always in opposition. The other film shown in Figure~\ref{fig:contradictions}, \textit{Gothika}, features two psychiatrists who are colleagues and friends. Miranda starts having paranormal experiences and is admitted in the hospital where they both work. Throughout the film, she tries to convince Pete that she is not mentally ill. As expected, Pete mostly expresses positive and later neutral sentiment as he tries to make Miranda aware of her situation, while Miranda is increasingly frustrated with Pete's lack of trust in her claims.


\section{Testing the main hypothesis}
\label{sec:predictors}
In order to test the main hypothesis, a number of prediction tasks were considered, including both classification and regression problems. The tasks themselves and the results are fully detailed in Chapter~\ref{chap:results}. Here, we will outline the main machine learning techniques used, the reasons for choosing them and their implementation. The toolkit used for carrying out machine learning tasks was the \textit{scikit-learn}\footnote{www.scikit-learn.org/stable/} Python library. It was used for its implementation of the predictors described below, for training and evaluating models using cross-validation and for generating confusion matrices. All models were evaluated on a 80\% / 20\% split between training and testing data, sampled randomly at each iteration.

\subsection{Classification}
We have used 3 common machine learning methods for our classification tasks. A common advantage of these methods is that they are (usually) computationally cheap to train and tend to produce good results without depending too much on optimising parameters. This enables our approach of exploring many different possibilities (detailed in Chapter~\ref{chap:results}) rather than focusing on a a small number of models and optimising their performance.
\begin{itemize}
	\item \textbf{Decision trees} work by training a rule-based model, which repeatedly splits the data set in order to maximise a certain measure (in our case, we used both information gain and Gini impurity). They can cope with un-normalised data and are not sensitive to outliers. Another usability advantage is that trees can be visualised and their rules examined. On the other hand, they sometimes tend to overfit and produce oversized trees (in principle, every point can be classified correctly if each node corresponds to a data point). Also, they can only produce axis-aligned splits of the data (since every node is a decision of whether to follow a certain branch based on how a particular feature compares to a certain value), which might be limiting in some applications. We used the \texttt{DecisionTreeClassifier} class implemented in \textit{scikit-learn}.
	\item \textbf{Random forests} \cite{breiman2001random} mitigate some of the shortcomings of decision trees. A random forest classifier will train a preset number of decision trees on different partitions of the data set and, for each new data point, output the most frequent prediction of the decision trees. This approach significantly ameliorates the decision trees' problems with overfitting, making random forest a viable and widely used predictors for many applications. However, it retains the decision trees' inability to generate non-axis-aligned splits. We used the \texttt{ensemble.RandomForestClassifier} class.
	\item \textbf{Support vector machines (SVMs)} \cite{cortes1995support} are a maximum-margin classifier, aiming to separate the data as cleanly as possible (creating the maximum margin between the class boundary and the closest data points on both sides). In addition to classifying linearly separable data, it can be used for nonlinear classification by using the ``kernel trick", which can transform the problem space into a higher-dimensional one using a non-linear kernel function. In this project, we tried using the linear, polynomial, RBF (radial basis function) and sigmoid kernels, as implemented in \textit{scikit-learn}, in the \texttt{svm} class. SVMs tend to produce reliable results, while avoiding overfitting, but they require more careful tuning than decision trees or random forests. Also, SVMs are by definition binary classifiers, meaning that in order to do multi-class predictions, a separate classifier needs to be trained for every pair of classes.
\end{itemize}

\subsection{Regression}
For regression, we have also used 4 common algorithms, 2 of which are also used in the classification tasks.
\begin{itemize}
	\item \textbf{Linear regression} is a simple regression model which fits the parameters of each feature using ordinary least squares to minimise the differences between actual and expected output. It is computationally cheap and often useful without much tuning, but it can be too rigid since it is a linear model (note that it is linear in the parameters, meaning that nonlinear transformations can be applied to the features). It is also sensitive to outliers, which we expect to affect the results in our case. We used the \texttt{linear\_model.LinearRegression} class.
	\item \textbf{Ridge regression} aims to improve on linear regression by enforcing a penalty on the size of the parameters, thus limiting the influence of outliers. However, the penalty parameter needs to be optimised. For this, we used the grid search method implemented in \textit{scikit-learn} as part of the \texttt{linear\_model.RidgeCV} class.
	\item \textbf{Decision tree regression} \cite{breiman1984classification} is an adaptation of decision trees to handle continuous output values. All the points from the discussion on decision tree classification above still apply. We used the \texttt{tree.DecisionTreeRegressor} class.
	\item \textbf{Support vector regression} is an adaptation of SVMs to handle regression problems. There are several methods to do this implemented in \textit{scikit-learn}, but we chose to use \texttt{svm.SVR}.
\end{itemize}

\section{Testing alternative hypotheses}
\label{sec:alt_hyp_implementation}

\subsection{Scraping the TV Tropes website}
The TV Tropes website does not offer an API for accessing its content. Therefore, in order to get the tropes for each film in the data set, we were required to implement a web scraper. A scraper is a piece of software which can automatically visit a large number of web pages, parse their DOM (Document Object Model) tree and extract relevant information. Scrapers can carry out more sophisticated tasks, but this was not required in our case.

We manually explored the structure of the TV Tropes website, which is quite consistent across the different pages, making our task easier. For working with the DOM inside Python, we used the Beautiful Soup library\footnote{www.crummy.com/software/BeautifulSoup}. We also had to manually match some of the film titles which differ slightly between ScriptBase and TV Tropes (e.g. \textit{Evil Dead II} and \textit{Evil Dead 2}).

A list of the most frequent 20 tropes across the films in our data set is shown in Table~\ref{table:tropes}, together with a discussion of why we could not exploit this data in our evaluation (Section~\ref{sec:alt_tropes}). Triangular relationship of the kind shown in Figure~\ref{fig:chasing_amy} above (\textit{Chasing Amy)} were the kinds of features we hoped to be able to predict. However, a very small number of films in our collection feature stereotypical 3-character relationships that would lend themselves to such a task.

\subsection{Comparing character graphs}
\label{sec:test_film_sim}

\subsubsection{Direct matching based on centrality}
As discussed in Section~\ref{sec:hyp_film_sim}, graph comparison is a difficult problem which requires careful consideration of the domain. When exploring the film similarity hypothesis, we implemented a simple comparison measure that is based on matching characters from different graphs by their role in the social network. Using degree centrality, the nodes of each graph are sorted and only the first 10 nodes are kept. This parameter can in principle be experimentally adjusted, but we have set it to 10 because for every film in the collection, we could not compute a persona for any character that has rank below 10 by degree centrality. This is not surprising, since plot summaries will rarely mention characters of low importance (assuming importance in the social network translates as importance to the narrative). We have kept this filtering of nodes for all the other tasks described in the report.

Then, every pair of corresponding characters (by rank) in the two graphs is compared. Note that although a persona is a set of 3 distributions, not all characters will have a distribution defined for all 3 types (agent, patient, modifier), since the relevant word lemmas might not be present in the plot summary. Therefore, we compare the distributions which both characters have in common, by taking the norm of the difference between the 2 vectors. The film similarity score is computed as the average of these differences: the lower the score, the more similar the graphs are deemed to be.

\begin{figure}[h]
\centering
\begin{minipage}{11.5cm}
\begin{Verbatim}[frame=single]
Star Trek V    - The Anniversary Party     (0.00167)
Ghost World    - I Love You Phillip Morris (0.00193)
Ghost World    - Copycat                   (0.00202)
Ghost World    - Clerks                    (0.00236)
Ghost World    - Miller's Crossing         (0.00257)
Ghost World    - Nick of Time              (0.00275)
Ghost World    - Sweeney Todd              (0.00338)
The Informant! - The Hangover              (0.00351)
Ghost World    - Gothika                   (0.00352)
Jaws           - Living in Oblivion        (0.00410)
\end{Verbatim}
\end{minipage}
\caption{The 10 most similar pairs of films in our collection of 96 thrillers and comedies, based on comparing personas matched by degree centrality rank}
\end{figure}

In these results, it is easy to see that the comedy \textit{Ghost World} (2001) dominates the similarity pairings, being matched both with comedies and thrillers. Overall, the results do not seem to be very revealing. To understand why this might be the case, let us explore the example of \textit{Ghost World}, by showing the top 10 characters and the top 3 topics for each character that has a persona defined. Each topic is described by the top 3 most probable words in that topic.

\begin{figure}[h]
\centering
\begin{minipage}{9cm}
\begin{Verbatim}[frame=single]
ENID 
REBECCA [ M(join wife know)
          M(pursue criminal escape)
          M(girl son offer) ]
SEYMOUR [ P(tell take go)
          A(tell take go)
          A(meet discover leave) ]
ROBERTA [ A(give kill confront)
          A(girl son offer)
          M(girl son offer) ]
JOE 
CUSTOMER 
PAUL 
GERROLD 
JEROME 
STEVEN 
\end{Verbatim}

\end{minipage}
\caption{The 10 most central characters in \textit{Ghost World} and their personas}
\end{figure}

One explanation for the dominance of \textit{Ghost World} in the similarity results shown above is the fact that its characters seem to be typical of both romantic comedies (topics such as \texttt{(join wife know)} or \texttt{(meet discover leave)}) and thrillers (topics like \texttt{(give kill confront)}).

We suspect that one of the reasons for the weak results is the small number of films in our collection and, consequently, the small number of words in the vocabulary. This can lead to a topic clustering which does not have much discriminative power (list of topics shown in Figure \ref{topics}). It might also be the case that, since not many personas are matched between films, these occasional matches are insufficient for obtaining meaningful results across the whole corpus.

\subsection{Rotten Tomatoes similarity}
In order to test the film similarity hypothesis, independent of the work described so far, we have also tried to construct a gold-standard collection of film similarity pairs, using data supplied by Rotten Tomatoes\footnote{www.rottentomatoes.com}. Their public API supports requests of the form "What films have users deemed to be similar to this one?" and offers up to 5 results.

We have found that the results obtained by querying every film in the collection and discarding every returned film which is not part of the collection yields a very small number of pairs. Thus, we have expanded the similarity measure by constructing a graph in which films are nodes and edges represent similarity and we also allow films from outside the corpus as nodes. The similarity can then be measured as the minimum distance in the graph.

However, the limited number of films in our collection and the fact that they are quite diverse (i.e. it is difficult to find pairs for which human judgements indicate similarity) meant that we could not meaningfully assess whether character graphs can be used to predict general film similarity. 

\chapter{Results and discussion}
\label{chap:results}

From its onset, this project was exploratory in nature. Experimenting with various film representations and the kinds of features that they can reliably predict was one of the main goals. As detailed in Chapter~\ref{chap:aims}, the main hypothesis we chose to explore is whether the character graph representation of films can be used to predict inter-character sentiment. The results presented in this chapter and the subsequent discussion will offer some insight into how this can be achieved. We will examine different ways in which persona information and character relationships can be combined. We ran a large number of experiments, exploring different transformations of the feature space and various machine learning techniques. This chapter will provide an overview of the types of prediction problems attempted, report interesting experimental results and attempt to discuss their significance.

\section{Experimental setup and data transformations}
The same experimental setup and basic data is used for all of the prediction tasks detailed below. The starting point is the set of 876 character-to-character relationships defined in our final collection of 86 films, described by the persona of each character and the weight of the edge between the nodes in the character graph. By considering relationships rather than films as our feature vectors we have increased the size of our data set ten-fold. Since our topic model has 10 topics and each persona is a set of 3 distributions over the latent topics, each feature vector will have size 61 ($2 \cdot 30$ persona variables $+ 1$ edge weight). Each vector will also have an associated label, depending on the prediction task, which will characterise some aspect of the sentiment between the two characters. The character relationships are directed, since we are attempting to capture the sentiment expressed by one character towards another. As a result, each pair of interacting characters will be represented by 2 feature vectors in the data set, with their persona distributions swapped.

\subsection{Data transformations}
Apart from using the the whole data set described above, we have also tried to explore ways in which the feature vectors can be transformed and data can be filtered in order to yield the most significant results. To this effect, we have designed and implemented the experimental setup in order to be able to answer questions such as:
\begin{enumerate}
	\item Are all 3 distributions of the persona model (\textit{agent}, \textit{patient}, \textit{modifier}) equally relevant?
	\item Can we get better predictions is we only use characters that have a more central role in the story?
	\item Should we use a uniform distribution for characters without persona information or should we exclude them altogether?
	\item Is it just a small number of topics that mostly define the persona of a character or are all 10 topics useful?
\end{enumerate}
And, naturally,
\begin{enumerate}
	\setcounter{enumi}{4}
	\item Does using the edge weight as a feature result in better predictions over simply using the persona model?
\end{enumerate}

When running experiments using the system developed for this project, the user can choose to either explore a certain data space (e.g. \textit{Make predictions using every combination of two persona distributions, rather than using all 3 of them}, which will result in 3 possible configurations), or to run the experiments on a predefined set of configurations. The next subsection explains what parameters can be specified when doing so\footnote{The code which facilitates this is found in the \texttt{main} method of \texttt{python/predictions/predict.py}}. 

\subsubsection{Presentation of results}
When presenting prediction results in the remainder of this chapter, they will be accompanied by the context in which they were obtained. The purpose of this section is to explain the meaning of the columns under the \textit{Configuration} heading in the results tables:
\begin{itemize}
	\item \textit{Number of nodes ($N$):} The (maximum\footnote{The number could be less than the one shown in a configuration where nodes without a persona are filtered out}) number of characters from each film that were used. These were selected based on their degree centrality in the character graph. Its value can either be \textit{All} or a number from 2 to 5.
	\item \textit{Filter personas:} \textit{Yes} or \textit{No} value, depending on whether nodes without persona information were filtered out, or their personas were filled in with uniform distributions.
	\item \textit{Personas used:} Shows which of the 3 distributions in a character persona were used. Can be any non-empty subset of $\{A, P, M\}$ (standing for \textit{agent}, \textit{patient} and \textit{modifier}).
	\item \textit{Top topics only ($T$):} This value can be either \textit{All topics}, showing that the persona distributions were used unmodified, or a number $n$ between 1 and 7, meaning that, for each distribution, the top $n$ topics by probability were selected and their values in the feature vectors was changed to 1, while the rest were set to 0, effectively creating a \textit{1-of-N} representation.
	\item \textit{Edge used:} \textit{Yes} or \textit{No} value, showing whether the edge weight between two characters was added as a feature or not.
\end{itemize}

\section{Predicting inter-character sentiment}

In this section, we present the results of 3 prediction tasks: 2 classification tasks and 1 regression. The first subsection will also give more details on how the data set can be transformed and filtered, by showing results for the full data set and then gradually varying the parameters described above.

\subsection{Predicting sentiment polarity}
First, we consider the problem of predicting compound sentiment polarity, modelled as a 2-class classification task. As discussed in Section~\ref{sec:predictors}, the classifiers used are decision trees, random forests and SVMs. In the case of random forests, 20 classifiers were trained on each configuration and the best one was selected.

Sentiment polarity of a relationship is computed as the sign of the cumulative sentiment expressed by one character towards the other. All the events between the 2 characters are summed across the sentiment trajectory of a film. If the sum is negative then the relationship will be in class 0 (\textit{negative}), otherwise, it will be in class 1 (\textit{positive}).

For this task, the baseline we are comparing our predictors against is the proportion of positive relationships in the total number of character relationships, which is always the majority class. As can be seen in the results tables below, they usually significantly outnumber the negative sentiment relationships, making this the simplest realistic baseline to compare our predictors against (i.e. comparing to a model which classifies every data point as having class 1). Models performing better than this baseline are highlighted in the tables.

We will first try to use the entire data set of 876 character relationships, with all 3 persona distributions across all topics (Table~\ref{res:full_set}). Decision trees do not manage to out-perform the baseline, while random forests and SVMs are narrowly more accurate than it, but the difference is never more than 4\%. When using the data set in which characters with missing personas are not filtered out, but their personas are treated as uniform distributions over the latent topics, no predictor manages to beat the baseline by more than 1\%. This is the case for all the subsequent prediction tasks, therefore we will skip those results. Another aspect to note is that for both random forests and SVMs, using the edge weight either improved or had no effect on the accuracy.

\begin{table}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c!{\vrule width 1.5pt}c|c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & & \\
Classifier & nodes & personas & used & only & used & Baseline & \textbf{Accuracy} \\ \hline
\multirow{4}{*}{Decision tree} 
 & All & Yes & A, P, M & All topics & Yes & 0.733 & 0.648\\
 & All & Yes & A, P, M & All topics & No & 0.733 & 0.670\\ 
 & All & No & A, P, M & All topics & Yes & 0.871 & 0.872\\
 & All & No & A, P, M & All topics & No & 0.871 & 0.871\\ \hline
\multirow{4}{*}{Random forests} 
 & All & Yes & A, P, M & All topics & Yes & 0.733 & \textbf{0.773}\\
 & All & Yes & A, P, M & All topics & No & 0.733 & \textbf{0.767}\\ 
 & All & No & A, P, M & All topics & Yes & 0.871 & \textbf{0.878}\\
 & All & No & A, P, M & All topics & No & 0.871 & \textbf{0.877}\\ \hline
\multirow{4}{*}{SVMs} 
 & All & Yes & A, P, M & All topics & Yes & 0.733 & \textbf{0.761}\\
 & All & Yes & A, P, M & All topics & No & 0.733 & \textbf{0.761}\\ 
 & All & No & A, P, M & All topics & Yes & 0.871 & \textbf{0.879}\\
 & All & No & A, P, M & All topics & No & 0.871 & \textbf{0.879}\\ \hline
\end{tabular}
\caption{Sentiment polarity predictions obtained by using the entire dataset and the full persona representation.}
\label{res:full_set}
\end{adjustwidth}
\end{table}

We will now try to train the same classifiers with different configurations of the data. First, we will remove the restriction that all characters in a film need to be used. We expect this to increase accuracy because some of the minor characters have less accurate persona information since they are less likely to be mentioned in a plot summary. Furthermore, their sentiment trajectories will have significantly fewer events, limited by the number of on-screen interactions that they are involved in. We will explore the range between 2 and 5 as the maximum number of characters from each film. The nodes are selected according to their rank in the character graph, which is the reverse order of degree centrality.

Table~\ref{res:diff_nodes} shows that the best results were obtained when limiting the number of characters to either 2 or 3. The results for all 3 classifiers are significantly better than those obtained when using all the characters.

\begin{table}[ht!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c!{\vrule width 1.5pt}c|c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & & \\
Classifier & nodes & personas & used & only & used & Baseline & \textbf{Accuracy} \\ \hline
\multirow{2}{*}{Decision tree} 
 & 2 & Yes & A, P, M & All topics & Yes & 0.713 & \textbf{0.750}\\
 & 2 & Yes & A, P, M & All topics & No & 0.713 & \textbf{0.750}\\ \hline
\multirow{2}{*}{Random forest} 
 & 2 & Yes & A, P, M & All topics & Yes & 0.713 & \textbf{0.875}\\
 & 3 & Yes & A, P, M & All topics & No & 0.699 & \textbf{0.882}\\ \hline
\multirow{2}{*}{SVM}
 & 3 & Yes & A, P, M & All topics & Yes & 0.699 & \textbf{0.824}\\ 
 & 3 & Yes & A, P, M & All topics & No & 0.699 & \textbf{0.824}\\ \hline
\end{tabular}
\caption{Best sentiment polarity prediction accuracy obtained when varying the maximum number of characters for each film.}
\label{res:diff_nodes}
\end{adjustwidth}
\end{table}

Next, we will both vary the maximum number of characters in the film, as before, and let the persona include any subset of the 3 distributions. Results are shown in Table~\ref{res:diff_nodes_personas}. Note that for random forests, a third row of results (highlighted) is included in order to compare the accuracy of either using or not using edge weights on the same configuration that yielded the best performance in the previous task. This shows that adding the M (\textit{modifier}) distribution and using the edge weight have the same effect on the accuracy (an increase of 6.3\%).

\begin{table}[ht!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{|c|c|c|c|c|c!{\vrule width 1.5pt}c|c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & & \\
Classifier & nodes & personas & used & only & used & Baseline & \textbf{Accuracy} \\ \hline
\multirow{2}{*}{Decision tree} 
 & 2 & Yes & A, P, M & All topics & Yes & 0.713 & \textbf{0.750}\\
 & 2 & Yes & A, P, M & All topics & No & 0.713 & \textbf{0.750}\\ \hline
\multirow{3}{*}{Random forest} 
 & 2 & Yes & A & All topics & Yes & 0.713 & \textbf{0.938}\\
 & 2 & Yes & A, M & All topics & No & 0.713 & \textbf{0.938}\\ 
\rowcolor{gray!35}
{\cellcolor{white}} & 2 & Yes & A & All topics & No & 0.713 & \textbf{0.875} \\ \hline
\multirow{2}{*}{SVM}
 & 3 & Yes & A, P, M & All topics & Yes & 0.699 & \textbf{0.824}\\ 
 & 3 & Yes & A, P, M & All topics & No & 0.699 & \textbf{0.824}\\ \hline
\end{tabular}
\caption{Best sentiment polarity prediction accuracy obtained when varying the maximum number of characters for each film and allowing the persona to include any subset of the 3 distributions it contains.}
\label{res:diff_nodes_personas}
\end{adjustwidth}
\end{table}

Finally, we will also allow the persona representations to either include all topics, or to only select the $n$ most probable topics and ignore the rest. Results are shown in Table~\ref{res:vary_everything}. Note that the highlighted row has been included in order to compare the accuracy of SVMs on the same number of maximum characters (same baseline) with the best model from the previous task. The percentages in brackets indicate the change in accuracy between the current and the previous task, for cases where the best results are comparable (having the same baseline).

The accuracy of these models is generally better than with the previous configurations. It remains unchanged in the case of random forests, for which allowing the number of latent topics to vary did not result in an increase in accuracy. These are the best results obtained in the sentiment polarity classification tasks. For all the trained classifiers, they are significantly higher than the baseline, showing that inter-character sentiment polarity can be predicted reasonably accurate. Note that in the final configuration shown in Table~\ref{res:vary_everything}, decision trees can achieve the same accuracy as random forests. Such a decision tree is pictured in Figure~\ref{label} (on page 52). It does not seem to drastically overfit, although some nodes do contain only 1 sample.

\begin{table}[ht!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c!{\vrule width 1.5pt}c|c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & & \\
Classifier & nodes & personas & used & only & used & Baseline & \textbf{Accuracy} \\ \hline
\multirow{2}{*}{Decision tree} 
 & 2 & Yes & A & 6 & Yes & 0.713 & \textbf{0.813 (+6.3\%)}\\
 & 2 & Yes & A, M & 3 & No & 0.713 & \textbf{0.938}\\ \hline
\multirow{2}{*}{Random forest} 
 & 2 & Yes & A & All topics & Yes & 0.713 & \textbf{0.938} (+0.0\%)\\
 & 2 & Yes & A, M & All topics & No & 0.713 & \textbf{0.938} (+0.0\%)\\ \hline
\multirow{3}{*}{SVM}
 & 2 & Yes & A & 3 & Yes & 0.713 & \textbf{0.875}\\
 & 2 & Yes & A & 3 & No & 0.713 & \textbf{0.875}\\ 
\rowcolor{gray!35}
{\cellcolor{white}} & 3 & Yes & A & 1 & -- & 0.699 & \textbf{0.853 (+2.9\%)}\\ \hline
\end{tabular}
\caption{Best sentiment polarity prediction accuracy obtained when varying the maximum number of characters for each film, allowing the persona to include any subset of the 3 distributions it contains and varying the number of topics over which personas are distributed.}
\label{res:vary_everything}
\end{adjustwidth}
\end{table}

\subsection{Predicting changes in sentiment}
As can be seen in the graphs shown in Section~\ref{sec:trajectories}, it appears that the way inter-character sentiment evolves intuitively conveys certain aspects about the nature of the narrative and the characters which take part (e.g. in a romantic film, both of the main characters tend to express positive sentiment throughout the story). This suggests another prediction problem: can we predict how the inter-character sentiment varies over the course of the film? We model this task as a 3-class classification problem and divide the sentiment trajectory of a film in 2 halves. If, in a character relationship, the averaged sentiment expressed by the first character towards the second one in the first half of the film is smaller than the sentiment expressed in the second half by at least some threshold $\alpha$, then their relationship is in class 0 (\textit{increasing}). If the absolute difference between the 2 halves is smaller than $\alpha$ then it is in class 1 (\textit{constant}). Otherwise, it is in class 3 (\textit{decreasing}).

We have introduced the parameter $\alpha$, which controls the number of data points in class 1. A value of $\alpha=0$ will results in class 1 being empty. In setting its value, the goal will be to obtain a balanced split between the 3 classes, across all the values of $N$. The graph in Figure ~\ref{fig:alpha} shows the proportion of points in the majority class, across all the values of $N$ used in the previous task, for 3 values of $\alpha$.

\begin{figure}[h!]
\begin{adjustwidth}{-0.7in}{-0.7in}
	\centering
	\includegraphics[scale=0.55]{figures/variation_alpha2}
	\caption{Proportion of data points in the majority class, over different values for the maximum number of characters ($N$), for 3 settings of the parameter $\alpha$.}
\label{fig:alpha}
\end{adjustwidth}
\end{figure}

In most (5 out of 7) cases, $\alpha=0.07$ makes the lowest number of points to be in the majority class. Either increasing or decreasing $\alpha$ will generally increase this proportion. We will therefore use $0.07$ when labelling the data used to train the classifiers. Their performance is shown in Table~\ref{res:sent_variation}. The highest accuracy is again obtained using random forests and it is $0.867$. Note that although setting $\alpha$ to $0.06$ or $0.08$ will result in a drop in performance,  in both of those cases, the best accuracy still ranges between $0.75$ and $0.80$.

\begin{table}[h!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c!{\vrule width 1.5pt}c|c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & & \\
Classifier & nodes & personas & used & only & used & Baseline & \textbf{Accuracy} \\ \hline
\multirow{2}{*}{Decision tree} 
 & 2 & Yes & P, M & 1 & Yes & 0.427 & \textbf{0.600}\\
 & 2 & Yes & P, M & 1 & No & 0.427 & \textbf{0.667}\\ \hline
\multirow{2}{*}{Random forest} 
 & 2 & Yes & P & 7 & Yes & 0.427 & \textbf{0.800}\\
 & 2 & Yes & A, P, M & 6 & No & 0.427 & \textbf{0.867}\\ \hline
\multirow{2}{*}{SVM}
 & 2 & Yes & P & 4 & Yes & 0.427 & \textbf{0.733}\\
 & 2 & Yes & P & 4 & No &  0.427 & \textbf{0.733}\\ \hline
\end{tabular}
\caption{Best sentiment variation prediction accuracy, with $\alpha = 0.07$.}
\label{res:sent_variation}
\end{adjustwidth}
\end{table}

We have shown that predicting changes in inter-character sentiment is also feasible with our representation. Note that the baselines are notably smaller than in the previous task, since we sought to obtain roughly equally sized classes.

\subsubsection{Comparison of results between the 2 classification tasks}
When predicting changes in sentiment, for all 3 classifiers, the best accuracy is obtained when limiting the maximum number of nodes in a character graph ($N$) to 2. This is similar to the sentiment polarity classification task, in which the best results were obtained when using an $N \in \{2, 3\}$. We can offer a possible explanation for this bias in both of these cases. The sentiment trajectories of films will naturally include more events between the more central characters in the narrative, which can result in less erratic predictions, since the influence of outliers (e.g. from shortcoming in sentiment analysis) is limited.

Another common feature to note is the preference, in both tasks, to one of the 3 distributions in a character persona. While the choice of distributions varies for different classifiers, we can observe that in the sentiment polarity prediction task all the best-performing models were trained using (sometimes exclusively) the \textit{agent} distribution. This kind of preference is also observed in the sentiment variation task, but for the \textit{patient} distribution. We can speculate that such a bias seems to show that actions that the characters do themselves will better predict the overall sentiment of a relationship, while actions which the characters experience are more likely predictors of how relationships evolve in the timeline of the story.

One more thing to note is that for both tasks and for all classifiers, using the edge weight (i.e. number of scene co-occurrences of 2 characters) as an extra feature in the feature vectors either has no effect on or decreases classification accuracy. This would seem to indicate that the persona model is enough to predict inter-character sentiment. However, as earlier discussed, the best results were obtained for values of $N \le 3$, with nodes ranked by their degree centrality in the character graph. As a consequence, it is possible that using the edge weight only added noise to the data and the inter-character sentiment depends more on how central the characters are to the story than on an absolute measure of the frequency of their interactions. This claim can be supported by the fact that using the edge weight did not decrease accuracy when all the characters of a film were included (Table~\ref{res:full_set}).

Another aspect which might seem surprising is the consistent better performance of random forests over SVMs, which are regarded to be a more sophisticated model. A often-cited study \cite{caruana2006} found that, on average, random forests do tend to perform better than SVMs, when averaging performance across a wide array of problems. This is in part due to the fact that SVMs are more sensitive to the effects of using sub-optimal parameters.

\subsection{Predicting compound sentiment}
We have modelled previous tasks as classification problems. However, inter-character sentiment is a continuous numerical value, meaning we can also explore regression tasks. The broadest such task is simply predicting the value of inter-character compound sentiment. Attempting to obtain the best possible results, we will explore the whole space of configurations introduced in the sections above. Results are shown in Table~\ref{res:regression}. 

\begin{table}[ht!]
\begin{adjustwidth}{-0.5in}{-0.5in}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c!{\vrule width 1.5pt}c| }
\cline{1-6}
\multicolumn{6}{ |c!{\vrule width 1.5pt} }{Configuration}\\
\hline
 & No. & Filter & Personas & Top topics & Edge & \textbf{Mean squared}\\
Classifier & nodes & personas & used & only & used & \textbf{error} \\ \hline
\multirow{2}{*}{Linear regression} 
 & All & Yes & M & 7 & Yes & 10.19\\
 & All & Yes & M & 7 & No & 10.19\\ \hline
\multirow{2}{*}{Ridge regression} 
 & All & Yes & A, M & 7 & Yes & 10.30\\
 & All & Yes & A, M & 7 & No & 10.28\\ \hline
\multirow{2}{*}{\parbox{2.3cm}{Decision tree regression}}
 & All & Yes & A & 2 & Yes & 10.55\\
 & All & Yes & A & 2 & No & 10.50\\ \hline
\multirow{2}{*}{SVM regression}
 & All & Yes & A & 3 & Yes & 10.99\\
 & All & Yes & A & 3 & No & 10.99\\ \hline
\end{tabular}
\caption{Best sentiment polarity prediction accuracy.}
\label{res:regression}
\end{adjustwidth}
\end{table}

Prediction results are very poor, with all the predictors achieving the mean squared error of around 10.0. The value of cumulative sentiment for the films in our data set ranges between -18.0 and +20.0. Predicting sentiment value was quite an ambitious task. The reasons behind poor performance, can, in addition to those discussed for the classification task, be the small number and diversity of films used, sub-optimal parameters (particularly for ridge regression and SVM regression) or the fact that the value of sentiment can be more sensitive to features of the language (rather than a character's persona) and more easily skewed by outliers than sentiment polarity and sentiment variance.

\includepdf[angle=90, addtolist={1, figure, Best decision tree trained for the sentiment polarity task\, with an accuracy of 93.8\%, label }]{figures/decision_tree}

\chapter{Conclusions and future work}

We have presented a novel representation of films, as a graph which models characters and their interactions. We also gave an overview of the previous work which guided the design and implementation decisions taken in this project and discussed the details and suitability of the different methods chosen. We have constructed and evaluated the character graphs using 86 thriller and comedy films, based on their ability to predict various features of the inter-character sentiment. We ran several experiments on different character graph variations and different subsets of the data set and found the representations and the prediction tasks that achieve the greatest accuracy. Although it seems that using the graph edge weight as a feature in our representation slightly hinders prediction accuracy in many cases, we have shown the viability of our simplified Dirichlet Persona Model, adapted from the work of Bamman et al. \cite{Bamman2013}.

\section{Future work}

Of course, in our exploration, we have only scratched the surface of this problem. At every step, there were many possibilities of doing things differently, which we did not have a chance to try in the scope of an honours project. A non-exhaustive list would include:
\begin{itemize}
	\item evaluate our character graphs using the original Dirichlet Persona Model and the Persona Regression Model in addition to our simplified persona model
	\item spend more time optimising the parameters of machine learning models such as SVM and ridge regression, which could potentially improve their performance significantly
	\item use other, more established, sentiment analysis systems, such as SentiWordNet\footnote{www.sentiwordnet.isti.cnr.it} or LIWC\footnote{www.liwc.net} and compare the results against those obtained with \textsc{VADER}
	\item rigorously evaluate the sentiment trajectories that we constructed; in the project, we only evaluated them by exemplifying how they can convey some aspects of the narrative on a selection of films
	\item process more films scripts in order to test other hypotheses; although not mentioned in the report, we attempted this, but the task proved very laborious and not easily automated, requiring manual annotation of each script
	\item explore more variations of the data space; in particular, vary the number of topics used for Latent Dirichlet Allocation; training LDA is quite computationally expensive (especially on more than 10.000 plot summaries), so we did not attempt too many possibilities
	\item as an alternative to the character graph model proposed, we could have used sentiment analysis to enhance either the character personality model or the edge weight, and use a different evaluation measure
	\item implement a procedure for comparing character graphs; the work in \cite{elsner2012character} could be a starting point
\end{itemize}

\section{Final thoughts}
I would like to end this report with a few remarks on the difficulty of computationally analysing works of fiction, by discussing a very recent and hotly debated example. In the Introduction, we touched upon the belief that stories can, on a high level, be generalised and that the gist of many successful stories can be captured by a ``shape" or skeleton. Recent work by Matthew Jockers on the Syuzhet package\footnote{www.github.com/mjockers/syuzhet} aims to accomplish this: automatically extract the ``latent structure of narrative by means of sentiment analysis". The model was used on 40.000 novels. Jockers claims that when comparing the shape of these novels, 6 or 7 distinct story archetypes emerge\footnote{Matthew Jockers. \textit{The Rest of the Story}. Blog post. 25 February 2015. www.matthewjockers.net/2015/02/25/the-rest-of-the-story. Note that this and all the following blog posts have been accessed on 1 April 2015.}. 

Although the paper presenting his approach has not yet been published (it is due to appear later this year), Syuzhet has received some media attention and has attracted discussion and criticism from researchers in the NLP and DH (digital humanities) communities, mostly expressed in exchanges of tweets and blog posts. Annie Swafford raises a number of concerns, including doubts about the accuracy of the sentence splitting technique employed and on the choice of smoothing function applied to the ``emotional trajectories" of the novels\footnote{Annie Swafford. \textit{Problems with the Syuzhet Package}. Blog post. 2 March 2015. www.annieswafford.wordpress.com/2015/03/02/syuzhet/ }. She also points out the limitations of using a number of unsophisticated sentiment analysis lexicons (among others, AFINN-96, discussed in Section~\ref{sec:trajectories}) when computing sentence-level sentiment, including the problem with simply summing the valences of words in a sentence, the loss of multiple word senses by assigning a word only one sentiment valence and the fact that these lexicons are constructed with modern English in mind, while the novels analysed with Syuzhet span a much longer period of time. Others, such as Andrew Piper, have contributed to the debate by touching on the inherent difficulty of rigorously validating or invalidating a hypothesis about subjective constructs such as sentiment or the shape of stories\footnote{Andrew Piper. \textit{Validation and Subjective Computing}. Blog post. 25 March 2015. www.txtlab.org/?p=470}. An interesting point (which we earlier discussed in our assessment of sentiment analysis in Section~\ref{sec:bg_sentiment}) is that evaluating the accuracy of a procedure in this context will invariably depend on what humans agree to be correct. This is an issue in many common NLP tasks (coreference resolution, named entity recognition, even POS tagging, to a much lower extent), where human agreement is relatively low. One person's sentiment trajectory of a story will probably not match with the other's. David Bamman tried to address this point by having five people annotate each scene of \textit{Romeo and Juliet} with a sentiment valence between -5 and +5 via the Amazon Mechanical Turk crowdsourcing platform\footnote{David Bamman. \textit{Validity}. Blog post. 1 April 2015. http://www.davidbamman.com/?p=52}. Agreement was high for some scenes (e.g. the balcony scene), but less so for others, especially towards the end of the play.

Beyond any technical shortcomings that might have inadvertently affected the work described in this report, the results presented here are also ultimately subject to the same validity concerns that are being discussed in this ongoing debate. We have established some correlations and demonstrated the usefulness of character graphs, but it is important to keep in mind that deeper claims about the nature of narratives might be harder to support.


\bibliographystyle{plain}
\bibliography{collection}

\appendix
\include{appendix1}

\end{document}